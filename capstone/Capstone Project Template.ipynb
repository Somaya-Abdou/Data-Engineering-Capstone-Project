{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project analyzes 2 types of data ,upload them to s3 bucket , put them in tables\n",
    "in redshift using pipelines and query the data while on the redshift cluster .\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All imports and installs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "from boto.s3.key import Key\n",
    "import boto\n",
    "import boto.s3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import psycopg2\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project normalized tables are used as the tables used haven't a large number of columns and are increased \n",
    "vertically so dimentional models aren't needed .\n",
    "\n",
    "Here data includes immigrants ids, gender, visa type, address, citizenship, date of arrival and travel methods.\n",
    "\n",
    "The end solution aims at finding the most flouded countries with immigrants ,their gender and citizenship.\n",
    "\n",
    "Apache spark , apache airflow , postgresql , s3 bucket and redshift cluster are used in this project\n",
    "\n",
    "#### Describe and Gather Data \n",
    "5 data sets are used in this project : 4 are csv files and 1 is a parquet file.\n",
    "\n",
    "data files are : I94cntyl_country,I94mode_travel,I94visa_type,addr csv_files\n",
    "\n",
    " :sas_data parquet_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here using configparser to get the secret and access key for the aws account and \n",
    "data needed to access the redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('capstone.cfg')\n",
    "KEY         = config.get('AWS','KEY')\n",
    "SECRET      = config.get('AWS','SECRET')\n",
    "BUCKET_NAME = config.get('CLUSTER','BUCKET_NAME')\n",
    "DB_NAME     = config.get('CLUSTER','DB_NAME')\n",
    "DB_USER     = config.get('CLUSTER','DB_USER')\n",
    "DB_PASSWORD = config.get('CLUSTER','DB_PASSWORD')\n",
    "DB_PORT     = config.get('CLUSTER','DB_PORT')\n",
    "DB_ENDPOINT = config.get('CLUSTER','DB_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "loading spark session and grabing sas_data parquet folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "config = configparser.ConfigParser()\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark =spark.read.load('./sas_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "loading all data sets used in this project to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark= spark.read.parquet('sas_data')\n",
    "country=spark.read.csv('I94cntyl_country.csv' , inferSchema= True)\n",
    "visa=spark.read.csv('I94visa_type.csv' ,  inferSchema= True)\n",
    "travel=spark.read.csv('I94mode_travel.csv' ,  inferSchema= True)\n",
    "address=spark.read.csv('addr.csv' ,  inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "many unwanted cloumns, null values and wrong type columns\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "Here, many unneeded columns,null values and duplicates from sas_data were removed and numeric values in other files are \n",
    "cast to double type also arr_date is cast to date type using udf to be able to extract year and month from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|  _c0|                 _c1|\n",
      "+-----+--------------------+\n",
      "|582.0| 'MEXICO Air Sea ...|\n",
      "+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+----------+\n",
      "|_c0|       _c1|\n",
      "+---+----------+\n",
      "|  1|'Business'|\n",
      "|  2|'Pleasure'|\n",
      "|  3| 'Student'|\n",
      "+---+----------+\n",
      "\n",
      "+---+-----+\n",
      "|_c0|  _c1|\n",
      "+---+-----+\n",
      "|  1|'Air'|\n",
      "+---+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+---------+\n",
      "|_c0|      _c1|\n",
      "+---+---------+\n",
      "| AL|'ALABAMA'|\n",
      "+---+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country.show(1)\n",
    "visa.show(4)\n",
    "travel.show(1)\n",
    "address.show(1)\n",
    "df_spark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country.printSchema()\n",
    "visa.printSchema()\n",
    "travel.printSchema()\n",
    "address.printSchema()\n",
    "df_spark.printSchema()\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa = visa.withColumn(\"_c0\",visa._c0.cast('double'))\n",
    "travel = travel.withColumn(\"_c0\",travel._c0.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_data = df_spark.drop('visapost','occup','entdepu','insnum','count','entdepd','matflag','i94yr','i94mon','i94port','i94bir','dtadfile','entdepa','dtaddto','insnum','airline','admnum','fltno','visatype').drop_duplicates().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_to_datetime(date):\n",
    "    \"\"\"\n",
    "    Convert to yyyy-mm-dd format\n",
    "    \n",
    "    :return: date in yyyy-mm-dd format\n",
    "    \"\"\"   \n",
    "    if date is not None:\n",
    "        return pd.Timestamp('1960-1-1')+pd.to_timedelta(date, unit='D')\n",
    "convert_to_datetime_udf = udf(convert_to_datetime, DateType())\n",
    "sas_data = sas_data.withColumn('arrdate', convert_to_datetime_udf(col('arrdate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: double (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- arrdate: date (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2431703"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country.printSchema()\n",
    "visa.printSchema()\n",
    "travel.printSchema()\n",
    "address.printSchema()\n",
    "sas_data.printSchema()\n",
    "sas_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "normalized data model is used here as data isn't that large to need dimentional model as star schema model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Data are uploaded to a s3 bucket then using airflow and a redshift cluster to create tables and load data into them\n",
    "to do queries on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### uploading data to s3 bucket\n",
    "if the bucket is publicly accessable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_data.write.mode(\"overwrite\").parquet(path=\"s3a://{}/sas_data/\".format(BUCKET_NAME))\n",
    "country.write.csv(\"s3a://{}/country.csv\".format(BUCKET_NAME))\n",
    "visa.write.csv(\"s3a://{}/visa.csv\".format(BUCKET_NAME))\n",
    "travel.write.csv(\"s3a://{}/travel.csv\".format(BUCKET_NAME))\n",
    "address.write.csv(\"s3a://{}/address.csv\".format(BUCKET_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### uploading data to s3 bucket\n",
    "\n",
    "if the bucket isn't publicly accessable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sas_data.write.parquet(\"sas_data_modified\")\n",
    "country.write.csv(\"country_modified\",header = True)\n",
    "visa.write.csv(\"visa_modified\",header = True)\n",
    "travel.write.csv(\"travel_modified\",header = True)\n",
    "address.write.csv(\"address_modified\",header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"sas_data_modified\", ignore_errors=False, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    "                       )\n",
    "s3 = session.resource('s3')\n",
    "bucket_name = BUCKET_NAME\n",
    "s3.meta.client.upload_file(Filename='country_modified/part-00000-cdfad2ea-b3ae-49b7-97a3-769d1b623d20-c000.csv', Bucket=bucket_name, Key='country.csv')\n",
    "s3.meta.client.upload_file(Filename='address_modified/part-00000-0aeebf1d-4890-4af7-8940-df39aa0a1e53-c000.csv', Bucket=bucket_name, Key='address.csv')\n",
    "s3.meta.client.upload_file(Filename='travel_modified/part-00000-d5f35a38-7205-4ae8-9548-d3a39de4daed-c000.csv', Bucket=bucket_name, Key='travel.csv')\n",
    "s3.meta.client.upload_file(Filename='visa_modified/part-00000-0af59931-38df-4b5c-a12b-a1e109c9f821-c000.csv', Bucket=bucket_name, Key='visa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+----------+-------+-------+-------+-------+-------+------+\n",
      "|cicid|i94cit|i94res|   arrdate|i94mode|i94addr|depdate|i94visa|biryear|gender|\n",
      "+-----+------+------+----------+-------+-------+-------+-------+-------+------+\n",
      "| 91.0| 103.0| 103.0|2016-04-01|    1.0|     IN|20551.0|    1.0| 1974.0|     F|\n",
      "+-----+------+------+----------+-------+-------+-------+-------+-------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"s3a://{}/sas_data\".format(BUCKET_NAME)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "data is built using airflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### If not using s3 bucket or airflow\n",
    "these are codes to create and load tables in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country.createOrReplaceTempView(\"country\")\n",
    "visa.createOrReplaceTempView(\"visa\")\n",
    "travel.createOrReplaceTempView(\"travel\")\n",
    "sas_data.createOrReplaceTempView(\"sas_data\")\n",
    "address.createOrReplaceTempView(\"address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Drop_table_staging_sas_data = '''Drop table if exists data.staging_sas_data'''\n",
    "Drop_table_staging_country = '''Drop table if exists data.staging_country'''\n",
    "Drop_table_staging_visa = '''Drop table if exists data.staging_visa'''\n",
    "Drop_table_staging_travel = '''Drop table if exists data.staging_travel'''\n",
    "Drop_table_staging_address = '''Drop table if exists data.staging_address'''\n",
    "Drop_table_immigrants = '''Drop table if exists data.immigrants'''\n",
    "Drop_table_address = '''Drop table if exists data.address'''\n",
    "Drop_table_date = '''Drop table if exists data.date'''\n",
    "\n",
    "create_schema = '''Create schema data '''\n",
    "\n",
    "create_staging_sas_data = ''' create table if not exists data.staging_sas_data (\n",
    "                                                         cicid float ,\n",
    "                                                         i94cit float,\n",
    "                                                         i94res float,\n",
    "                                                         arrdate date,\n",
    "                                                         i94mode float,\n",
    "                                                         i94addr string,\n",
    "                                                         depdate float,\n",
    "                                                         i94visa float,\n",
    "                                                         biryear float,\n",
    "                                                         gender string ) '''\n",
    "create_staging_country = ''' create table if not exists data.staging_country (\n",
    "                                                         i94cntyl float ,\n",
    "                                                         country string)'''\n",
    "create_staging_visa = ''' create table if not exists data.staging_visa (\n",
    "                                                         i94visa float ,\n",
    "                                                         visa_value string)'''\n",
    "create_staging_travel = '''Create table if not exists data.staging_travel (\n",
    "                                                            i94mode float,\n",
    "                                                            travel_mode string)'''\n",
    "create_staging_address = ''' create table if not exists data.staging_address(\n",
    "                                                                i94addr string,\n",
    "                                                                place string) '''\n",
    "create_immigrants_table ='''create table if not exists data.immigrants (\n",
    "                                                       citizen_id float ,\n",
    "                                                       country_cit string,\n",
    "                                                       birth_year float,\n",
    "                                                       gender string)'''\n",
    "create_address_table = '''create table if not exists data.address (\n",
    "                                                       citizen_id float ,\n",
    "                                                       address string,\n",
    "                                                       visa_type string ,\n",
    "                                                       travel_mode string)'''\n",
    "create_date_table = '''create table if not exists data.date (\n",
    "                                                  citizen_id float ,\n",
    "                                                  arrival_date date,\n",
    "                                                  arrival_day int,\n",
    "                                                  arrival_month int,\n",
    "                                                  arrival_year int\n",
    "                                                                    )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_tables = [create_staging_sas_data,create_staging_country,create_staging_visa,create_staging_travel,create_staging_address,create_immigrants_table,create_address_table,create_date_table]\n",
    "drop_tables = [Drop_table_staging_sas_data,Drop_table_staging_country,Drop_table_staging_visa,Drop_table_staging_travel,Drop_table_staging_address,Drop_table_immigrants,Drop_table_address,Drop_table_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in drop_tables :\n",
    "    spark.sql(i)\n",
    "for i in create_tables :\n",
    "    spark.sql(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "load_staging_sas_data = (''' insert into data.staging_sas_data\n",
    "                                        select   s.cicid ,\n",
    "                                                 s.i94cit ,\n",
    "                                                 s.i94res ,\n",
    "                                                 s.arrdate ,\n",
    "                                                 s.i94mode ,\n",
    "                                                 s.i94addr ,\n",
    "                                                 s.depdate ,\n",
    "                                                 s.i94visa ,\n",
    "                                                 s.biryear ,\n",
    "                                                 s.gender\n",
    "                                        From sas_data as s       ''') \n",
    "load_staging_country =('''insert into data.staging_country \n",
    "                                         select c._c0 ,\n",
    "                                                c._c1 \n",
    "                                          From  country as c ''')\n",
    "load_staging_visa = (''' insert into data.staging_visa \n",
    "                                         select v._c0 ,\n",
    "                                                v._c1\n",
    "                                          From  visa as v   ''')\n",
    "load_staging_travel = (''' insert into data.staging_travel \n",
    "                                         select t._c0 ,\n",
    "                                                t._c1\n",
    "                                          From  travel as t ''')\n",
    "load_staging_address=('''insert into data.staging_address\n",
    "                                          select a._c0 ,\n",
    "                                                 a._c1\n",
    "                                          From  address as a ''')\n",
    "load_immigrants_table = ('''Insert into data.immigrants \n",
    "                                        select s.cicid,\n",
    "                                               c.country,\n",
    "                                               s.biryear,\n",
    "                                               s.gender\n",
    "                                        From  data.staging_sas_data s\n",
    "                                        left join data.staging_country c\n",
    "                                             ON c.i94cntyl = s.i94cit''')\n",
    "load_address_table = ('''Insert into data.address \n",
    "                                        select b.cicid,\n",
    "                                                  b.place,\n",
    "                                                  b.visa_value,\n",
    "                                                  t.travel_mode\n",
    "                                           From (select o.cicid,\n",
    "                                                     o.place,\n",
    "                                                     v.visa_value,\n",
    "                                                     o.i94visa,\n",
    "                                                     o.i94mode\n",
    "                                                From ( select s.cicid,\n",
    "                                                              a.place,\n",
    "                                                              s.i94visa ,\n",
    "                                                              s.i94mode\n",
    "                                                         From data.staging_sas_data s\n",
    "                                                         Left Join data.staging_address a\n",
    "                                                         ON s.i94addr = a.i94addr   \n",
    "                                                      ) as o\n",
    "                                                             Left Join data.staging_visa v\n",
    "                                                             ON v.i94visa = o.i94visa \n",
    "                                                 ) as b\n",
    "                                                                    Left Join data.staging_travel t\n",
    "                                                                    ON t.i94mode = b.i94mode\n",
    "                      ''')\n",
    "load_date_table = (''' Insert into data.date \n",
    "                                      select cicid,\n",
    "                                             arrdate,\n",
    "                                             date_format(date(arrdate),'dd') as day,\n",
    "                                             Month(arrdate) as month,\n",
    "                                             date_format(date(arrdate),'yyyy') as year\n",
    "                                      From   data.staging_sas_data      ''')\n",
    "                                \n",
    "load_tables =[load_staging_sas_data,load_staging_visa,load_staging_travel,load_staging_address,load_immigrants_table,load_address_table,load_date_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in load_tables :\n",
    "    spark.sql(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------------+------------+\n",
      "|citizen_id|arrival_date|arrival_day|arrival_month|arrival_year|\n",
      "+----------+------------+-----------+-------------+------------+\n",
      "|      41.0|  2016-04-01|          1|            4|        2016|\n",
      "|      88.0|  2016-04-01|          1|            4|        2016|\n",
      "|     337.0|  2016-04-01|          1|            4|        2016|\n",
      "|     411.0|  2016-04-01|          1|            4|        2016|\n",
      "|     412.0|  2016-04-01|          1|            4|        2016|\n",
      "|     684.0|  2016-04-01|          1|            4|        2016|\n",
      "|    1124.0|  2016-04-01|          1|            4|        2016|\n",
      "|    1270.0|  2016-04-01|          1|            4|        2016|\n",
      "|    1437.0|  2016-04-01|          1|            4|        2016|\n",
      "|    1481.0|  2016-04-01|          1|            4|        2016|\n",
      "+----------+------------+-----------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' select * from data.date ''').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " data checks in airflow check if each table has rows and has content in them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### running queries on the finished tables to see the output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n",
      "postgresql://awsuser:Shizoka1234@redshift-cluster-1.cvfmpkrnx51k.us-west-2.redshift.amazonaws.com:5439/dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: awsuser@dev'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%load_ext sql\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format( DB_USER, DB_PASSWORD,DB_ENDPOINT ,DB_PORT,DB_NAME)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "United Kingdom is the highest country of the number of travellers and most of them travel for pleasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@redshift-cluster-1.cvfmpkrnx51k.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "10 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>country_cit</th>\n",
       "        <th>gender</th>\n",
       "        <th>count</th>\n",
       "        <th>visa_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;UNITED KINGDOM&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>1843792</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;UNITED KINGDOM&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>1777888</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;JAPAN&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>1166496</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;FRANCE&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>1106928</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;MEXICO Air Sea and Not Reported (I-94 no land arrivals)&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>1076704</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;FRANCE&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>1012688</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;JAPAN&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>986624</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;CHINA ; PRC&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>964976</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;MEXICO Air Sea and Not Reported (I-94 no land arrivals)&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>870432</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;BRAZIL&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>820992</td>\n",
       "        <td>&#x27;Pleasure&#x27;</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(\"'UNITED KINGDOM'\", 'F', 1843792, \"'Pleasure'\"),\n",
       " (\"'UNITED KINGDOM'\", 'M', 1777888, \"'Pleasure'\"),\n",
       " (\"'JAPAN'\", 'F', 1166496, \"'Pleasure'\"),\n",
       " (\"'FRANCE'\", 'F', 1106928, \"'Pleasure'\"),\n",
       " (\"'MEXICO Air Sea and Not Reported (I-94 no land arrivals)'\", 'F', 1076704, \"'Pleasure'\"),\n",
       " (\"'FRANCE'\", 'M', 1012688, \"'Pleasure'\"),\n",
       " (\"'JAPAN'\", 'M', 986624, \"'Pleasure'\"),\n",
       " (\"'CHINA ; PRC'\", 'F', 964976, \"'Pleasure'\"),\n",
       " (\"'MEXICO Air Sea and Not Reported (I-94 no land arrivals)'\", 'M', 870432, \"'Pleasure'\"),\n",
       " (\"'BRAZIL'\", 'F', 820992, \"'Pleasure'\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "Select country_cit ,gender , count(country_cit),visa_type\n",
    "From data.immigrants i\n",
    "Join data.address d\n",
    "ON d.citizen_id = i.citizen_id\n",
    "group by 1,2,4\n",
    "order by 3 desc\n",
    "limit 10 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here shows that most travellers use air method and then land more than sea and\n",
    "most of them are men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://awsuser:***@redshift-cluster-1.cvfmpkrnx51k.us-west-2.redshift.amazonaws.com:5439/dev\n",
      "14 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>travel_mode</th>\n",
       "        <th>gender</th>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Air&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>19639040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Air&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>18409808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Land&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>365312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Land&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>325856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Sea&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>65088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Sea&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>61120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Not reported&#x27;</td>\n",
       "        <td>F</td>\n",
       "        <td>17440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Not reported&#x27;</td>\n",
       "        <td>M</td>\n",
       "        <td>14960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Air&#x27;</td>\n",
       "        <td>X</td>\n",
       "        <td>4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Land&#x27;</td>\n",
       "        <td>U</td>\n",
       "        <td>3104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Sea&#x27;</td>\n",
       "        <td>X</td>\n",
       "        <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Not reported&#x27;</td>\n",
       "        <td>X</td>\n",
       "        <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Land&#x27;</td>\n",
       "        <td>X</td>\n",
       "        <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>&#x27;Air&#x27;</td>\n",
       "        <td>U</td>\n",
       "        <td>128</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(\"'Air'\", 'M', 19639040),\n",
       " (\"'Air'\", 'F', 18409808),\n",
       " (\"'Land'\", 'M', 365312),\n",
       " (\"'Land'\", 'F', 325856),\n",
       " (\"'Sea'\", 'F', 65088),\n",
       " (\"'Sea'\", 'M', 61120),\n",
       " (\"'Not reported'\", 'F', 17440),\n",
       " (\"'Not reported'\", 'M', 14960),\n",
       " (\"'Air'\", 'X', 4128),\n",
       " (\"'Land'\", 'U', 3104),\n",
       " (\"'Sea'\", 'X', 624),\n",
       " (\"'Not reported'\", 'X', 496),\n",
       " (\"'Land'\", 'X', 144),\n",
       " (\"'Air'\", 'U', 128)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select a.travel_mode ,i.gender , count(i.gender)\n",
    "from data.address a\n",
    "join data.immigrants i\n",
    "ON i.citizen_id = a.citizen_id\n",
    "group by 1,2\n",
    "order by 3 desc ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Spark is used here as an easy and fast etl choice to handle the data however large the become and by using airflow and dags \n",
    "any mistake can be spotted easily along the way and the time of production is controlled.\n",
    "Also using a normalized schema is suitable for the size of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It is more appropiate to update this data once a month as they are not that large or too dependant on time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### For the 1st senario\n",
    "if the data was increased vertically then the change would only be in the size of the cluster but if the\n",
    "increase is vertically then a star schema approach is better in that case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### For the 2nd senario\n",
    "then in the dag of the pipeline we edit the start time and schedule interval to be daily at 7 am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### For the 3rd senario\n",
    "If the data to be accessed by more than 100 people then it will be stored on s3 bucket that has a public access\n",
    "with ACLs mode not to depend on aws accounts "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
